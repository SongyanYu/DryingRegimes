sinpi(pi/2)
sinpi(180*(pi/180))
sinpi(360*(pi/180))
sin(180*pi)
rad(360)
sin(rad(360))
sin(rad(0))
sin(rad(0.000001))
sin(2*pi)
sin(2*round(pi,4))
sin(180*(pi/180))
sin(181*(pi/180))
sin(182*(pi/180))
cos(120*pi/180)
cos(360*pi/180)
cos(0*pi/180)
cos(event$peak_date*pi/180)
cos(1*pi/180)
cos(2*pi/180)
sin(52.517*(pi/180))
cos(1*pi/180)
cos(344*pi/180)
cos(359*pi/180)
cos(1*pi/180)
cos(359*pi/180)
cos(2*pi/180)
cos(358*pi/180)
sin(event$peak_date*pi/180)
seq(0:24)
seq(0:24,24)
seq(0:24,1)
seq(0:24,from:0)
seq(0:24,from=0)
seq(0:23)
0:24
tt = 0:24
circ = circular::circular(tt,units="degrees")
circ
circ = circular::circular(tt,units="hours")
circ
tt = 0:360
circ = circular::circular(tt,units="degrees")
circ
plot(circ)
360/365
(360/365)/45
45/(360/365)
315*(pi/180)
315*fact
320*fact
event = read.csv('../data/metrics_by_event.csv')
event$peak_date_strans = event$peak_date * fact
event$peak_date_strans = sin((event$peak_date * fact)*(pi/180))
View(event)
var(event$peak_date)
unload(circular())
unload(circular
)
detach("package:circular", unload=TRUE)
var(event$peak_date)
summary(event$peak_date)
var(event$peak_date,rm.na=T)
var(event$peak_date,na.rm=T)
var(event$peak_date_strans)
var(event$peak_date_strans,na.rm=T)
var(scale(event$peak_date),na.rm=T)
var(scale(event$peak_date_strans),na.rm=T)
scale(event$peak_date_strans)
var(event$peak_date_strans,na.rm=T)
mean(event$peak_date,na.rm=T)
mean(event$peak_date,na.rm=T)/sd(event$peak_date,na.rm=T)
mean(event$peak_date_strans,na.rm=T)
mean(event$peak_date_strans,na.rm=T)/sd(event$peak_date_strans,na.rm=T)
# Event based dataset
event = read.csv('../data/metrics_by_event.csv') %>% na.omit()
library(tidyverse)
# Event based dataset
event = read.csv('../data/metrics_by_event.csv') %>% na.omit()
setwd("/Volumes/GoogleDrive/My Drive/12_subprojects/DryingRegimes/code")
# Event based dataset
event = read.csv('../data/metrics_by_event.csv') %>% na.omit()
# Ecoregion dataset
er.dat = read.csv('../data/gages_with_ecoregion.csv')
# Meta data and landuse
metdata = read.csv('../data/mean_annual_no_flow_and_climate_metric_means_for_no_flow_sites_082819_with_info.csv')
# Watershed storage characteristics from GAGES2
ws.dat = read.csv('../data/20200211_watershed_storage.csv')
# Percentile data for 909 subset gages
percentile.dat = read.csv('../data/gages_percentile.csv')
peak.ant = read.csv('../data/metrics_by_event_withAntCondPeak.csv')
noflow.ant = read.csv('../data/metrics_by_event_withAntCondNoFlow.csv')
alpha = 0.10
event.filt = event[event$drying_rate >0 & event$p_value<= alpha,]
event.filt = event[event$drying_rate >0,]
summary(event.filt$p_value)
hist(event.filt$p_value)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 1: Setup workspace ------------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Clear memory
remove(list=ls())
#Libraries (for Windows OS)
library(parallel)
library(clustsig)
library(vegan)
library(lubridate)
library(tidyverse)
#Load metrics into R env
df<-read.csv(paste0('../data/metrics_by_event_combined.csv'))
library(tidyverse)
library(mclust)
library(ggplot2)
library(mapdata)
library(ggfortify)
library(plotly)
library(ClusterR)
library(cluster)
library(factoextra)
library(GGally)
library(vegan)
library(clustsig)
library(dplyr)
library(patchwork)
library(parallel)
library("corrplot")
library(scales)
#################### Load  and filter data #################
df = read.csv("../data/metrics_by_event_combined.csv")
#################### Load  and filter data #################
df = read.csv("../data/metrics_by_event_combined_raw.csv")
df$Name[df$Name == "Ignore"] = "Mediterranean California"
df = df[df$peak_quantile>.25,]
df = df[df$peak_quantile>.25 & df$drying_rate>=0,]
summary(df$drying_rate)
#Rename event_id  (Somethign is weird here...)
df<-df %>%
mutate(event_id = seq(1, nrow(df)))
df = df %>% group_by(gage) %>% count() %>% left_join(df,.,by="gage")
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 2: Estimate events per year ---------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Create fun to estimate number of drying events in the same meterologic year per event
fun<-function(n){
#Libraries of interest
library(dplyr)
#isolate event of interest
event <- df[n,]
#count number of events in same year and at same gage
count<- df %>%
filter(meteorologic_year == event$meteorologic_year) %>%
filter(gage == event$gage) %>%
nrow()
#Export info
tibble(
event_id = event$event_id,
freq_local = count
)
}
#run function
n.cores <- detectCores() - 1
cl <- makeCluster(n.cores)
clusterExport(cl, "df")
output<-parLapply(cl, seq(1,nrow(df)), fun)
remove(cl)
#add results to df
output<-bind_rows(output)
df<-left_join(df,output)
### Make rel_freq metric
df$rel_freq = df$freq_local/df$n
dat.scale <- df %>%
#Select vars of interest
select("peak2zero","drying_rate",
"dry_date_start", "dry_dur",
"peak_quantile", "rel_freq") %>%
#scale vars
scale()
dat.scale <- df %>%
#Select vars of interest
select("peak2zero","drying_rate"
, "dry_dur",
"peak_quantile", "rel_freq") %>%
#scale vars
scale()
############## PCA ############
PCA = prcomp(dat.scale)
autoplot(PCA,loadings=T,loadings.label=T)
# Visualize eigenvalues/variances
fviz_screeplot(PCA, addlabels = TRUE, ylim = c(0, 50))
################### K-Means ######################
fviz_nbclust(dat.scale, kmeans, method = "silhouette") + theme_classic()
wcke<-eclust(dat.scale, "kmeans", hc_metric="euclidean",k=4)
wcke<-eclust(dat.scale, "kmeans", hc_metric="euclidean",k=5)
fviz_cluster(wcke, geom = "point", ellipse.type = "norm", ggtheme = theme_minimal())
opt_gmm = Optimal_Clusters_GMM(dat.scale, max_clusters = 10, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
gmm = GMM(dat.scale, 5, dist_mode = "maha_dist", seed_mode = "random_subset", km_iter = 10,
em_iter = 10, verbose = F)
pr = predict_GMM(dat.scale, gmm$centroids, gmm$covariance_matrices, gmm$weights)
# Create distance matrix with scaled vars
d <- dat.scale %>%
vegdist(., method = 'euclidean')
#Use heirchal clustering
fit <- hclust(d, method = "ward")
plot(fit,
sub="Sampling Site",
hang=-0.5,
main = NULL,
labels = FALSE,
ylab="Height")
#After visual inspection, select where to cut the tree
df<- df %>%
mutate(
clust_4 = cutree(fit, k=5))
plot(fit,
sub="Sampling Site",
hang=-0.5,
main = NULL,
labels = FALSE,
ylab="Height")
title("Cluster Analysis: Ward's Mimium Variance (Euclidean Distance)", line = 3, cex =2)
title("4 Groups", line = 2)
rect.hclust(fit, k=5)
#After visual inspection, select where to cut the tree
df<- df %>%
mutate(
clust_4 = cutree(fit, k=4))
plot(fit,
sub="Sampling Site",
hang=-0.5,
main = NULL,
labels = FALSE,
ylab="Height")
title("Cluster Analysis: Ward's Mimium Variance (Euclidean Distance)", line = 3, cex =2)
title("4 Groups", line = 2)
rect.hclust(fit, k=4)
##################### Density Based Clustering #########################
library(fpc)
library(dbscan)
library(factoextra)
kNNdistplot(dat.scale,k = log(length(dat.scale)))
db = fpc::dbscan(dat.scale,eps=.5,MinPts = log(length(dat.scale)))
fviz_cluster(db,dat.scale, stand = FALSE, ellipse = FALSE, geom = "point",show.clust.cent = T,pointsize = .5)
# write.csv(clust,"../data/clustering_results.csv")
######## Plots ##########
# Color scale
cols <-
c("1" = "#4477AA",
"2" = "#66CCEE",
"3" = "#228833",
"4" = "#CCBB44",
"5" = "#EE6677",
"6" = "#AA3377",
"7" = "#BBBBBB",
"8" = "#999944",
"9" = "#332288")
PCA = prcomp(dat.scale)
var <- get_pca_var(PCA)
corrplot(var$cos2, is.corr=FALSE)
df_out <- as.data.frame(PCA$x)
df_out$group <- sapply( strsplit(as.character(row.names(df)), "_"), "[[", 1 )
head(df_out)
k_means <- ggplot(df_out,aes(x=PC1,y=PC2,col=factor(clust$kmeans.clust)))+
geom_point(size=3,alpha=0.5)+
scale_color_manual(name = "K-means Cluster",values = cols)+
theme_classic()
gmm.clust <- ggplot(df_out,aes(x=PC1,y=PC2,col=factor(clust$gmm.clust)))+
geom_point(size=3,alpha=0.5)+
scale_color_manual(name = "GMM Cluster",values = cols)+
theme_classic()
hier <- ggplot(df_out,aes(x=PC1,y=PC2,col=factor(clust$hier.4.clust)))+
geom_point(size=3,alpha=0.5)+
scale_color_manual(name = "Hierarchical Cluster",values = cols)+
theme_classic()
dbscan.clust <- ggplot(df_out,aes(x=PC1,y=PC2,col=factor(clust$dbscan.cluster)))+
geom_point(size=3,alpha=0.5)+
scale_color_manual(name = "DBSCAN Cluster",values = cols)+
theme_classic()
# Stats plot
dat.metrics = df %>% select("peak2zero","drying_rate",
"dry_date_start", "dry_dur",
"peak_quantile", "rel_freq")
h  = ggpairs(dat.metrics,
aes(color = as.factor(clust$hier.4.clust)))
for(i in 1:h$nrow) {
for(j in 1:h$ncol){
h[i,j] <- h[i,j] +
scale_fill_manual(values=cols) +
scale_color_manual(values=cols)
}
}
g = ggpairs(dat.metrics,
aes(color = as.factor(clust$gmm.clust)))
for(i in 1:g$nrow) {
for(j in 1:g$ncol){
g[i,j] <- g[i,j] +
scale_fill_manual(values=cols) +
scale_color_manual(values=cols)
}
}
k = ggpairs(dat.metrics,
aes(color = as.factor(clust$kmeans.clust)))
for(i in 1:k$nrow) {
for(j in 1:k$ncol){
k[i,j] <- k[i,j] +
scale_fill_manual(values=cols) +
scale_color_manual(values=cols)
}
}
d = ggpairs(dat.metrics,
aes(color = as.factor(clust$dbscan.clust)))
for(i in 1:d$nrow) {
for(j in 1:d$ncol){
d[i,j] <- d[i,j] +
scale_fill_manual(values=cols) +
scale_color_manual(values=cols)
}
}
h
g
k
d
clust = df %>% select(gage,Name,CLASS,dec_lat_va,dec_long_va,clust_4,peak2zero,drying_rate,dry_date_start, dry_dur,peak_quantile, rel_freq)
clust = cbind(clust,pr$cluster_labels+1,wcke$cluster,db$cluster+1)
wcke<-eclust(dat.scale, "kmeans", hc_metric="euclidean",k=5)
clust = df %>% select(gage,Name,CLASS,dec_lat_va,dec_long_va,clust_4,peak2zero,drying_rate,dry_date_start, dry_dur,peak_quantile, rel_freq)
clust = cbind(clust,pr$cluster_labels+1,wcke$cluster,db$cluster+1)
wcke<-eclust(dat.scale, "kmeans", hc_metric="euclidean",k=5)
library(tidyverse)
library(mclust)
library(ggplot2)
library(mapdata)
library(ggfortify)
library(plotly)
library(ClusterR)
library(cluster)
library(factoextra)
library(GGally)
library(vegan)
library(clustsig)
library(dplyr)
library(patchwork)
library(parallel)
library("corrplot")
library(scales)
df = read.csv("../data/metrics_by_event_combined_raw.csv")
df$Name[df$Name == "Ignore"] = "Mediterranean California"
df = df[df$peak_quantile>.25 & df$drying_rate>=0,]
#Rename event_id  (Somethign is weird here...)
df<-df %>%
mutate(event_id = seq(1, nrow(df)))
df = df %>% group_by(gage) %>% count() %>% left_join(df,.,by="gage")
fun<-function(n){
#Libraries of interest
library(dplyr)
#isolate event of interest
event <- df[n,]
#count number of events in same year and at same gage
count<- df %>%
filter(meteorologic_year == event$meteorologic_year) %>%
filter(gage == event$gage) %>%
nrow()
#Export info
tibble(
event_id = event$event_id,
freq_local = count
)
}
#run function
n.cores <- detectCores() - 1
cl <- makeCluster(n.cores)
clusterExport(cl, "df")
output<-parLapply(cl, seq(1,nrow(df)), fun)
remove(cl)
#add results to df
output<-bind_rows(output)
df<-left_join(df,output)
### Make rel_freq metric
df$rel_freq = df$freq_local/df$n
dat.scale <- df %>%
#Select vars of interest
select("peak2zero","drying_rate",
"dry_date_start", "dry_dur",
"peak_quantile", "rel_freq") %>%
#scale vars
scale()
dat.scale <- df %>%
#Select vars of interest
select("peak2zero","drying_rate"
, "dry_dur",
"peak_quantile", "rel_freq") %>%
#scale vars
scale()
wcke<-eclust(dat.scale, "kmeans", hc_metric="euclidean",k=5)
rm(df)
rm(output)
wcke<-eclust(dat.scale, "kmeans", hc_metric="euclidean",k=5)
#################### Load  and filter data #################
df = read.csv("../data/metrics_by_event_combined.csv")
df$Name[df$Name == "Ignore"] = "Mediterranean California"
df = df[df$peak_quantile>.25 & df$drying_rate>=0,]
#Rename event_id  (Somethign is weird here...)
df<-df %>%
mutate(event_id = seq(1, nrow(df)))
df = df %>% group_by(gage) %>% count() %>% left_join(df,.,by="gage")
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 2: Estimate events per year ---------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Create fun to estimate number of drying events in the same meterologic year per event
fun<-function(n){
#Libraries of interest
library(dplyr)
#isolate event of interest
event <- df[n,]
#count number of events in same year and at same gage
count<- df %>%
filter(meteorologic_year == event$meteorologic_year) %>%
filter(gage == event$gage) %>%
nrow()
#Export info
tibble(
event_id = event$event_id,
freq_local = count
)
}
#run function
n.cores <- detectCores() - 1
cl <- makeCluster(n.cores)
clusterExport(cl, "df")
output<-parLapply(cl, seq(1,nrow(df)), fun)
remove(cl)
#add results to df
output<-bind_rows(output)
df<-left_join(df,output)
### Make rel_freq metric
df$rel_freq = df$freq_local/df$n
dat.scale <- df %>%
#Select vars of interest
select("peak2zero","drying_rate",
"dry_date_start", "dry_dur",
"peak_quantile", "rel_freq") %>%
#scale vars
scale()
dat.scale <- df %>%
#Select vars of interest
select("peak2zero","drying_rate"
, "dry_dur",
"peak_quantile", "rel_freq") %>%
#scale vars
scale()
wcke<-eclust(dat.scale, "kmeans", hc_metric="euclidean",k=5)
#################### Load  and filter data #################
df = read.csv("../data/metrics_by_event_combined_raw.csv")
df$Name[df$Name == "Ignore"] = "Mediterranean California"
df = df[df$peak_quantile>.25 & df$drying_rate>=0,]
#Rename event_id  (Somethign is weird here...)
df<-df %>%
mutate(event_id = seq(1, nrow(df)))
df = df %>% group_by(gage) %>% count() %>% left_join(df,.,by="gage")
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 2: Estimate events per year ---------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Create fun to estimate number of drying events in the same meterologic year per event
fun<-function(n){
#Libraries of interest
library(dplyr)
#isolate event of interest
event <- df[n,]
#count number of events in same year and at same gage
count<- df %>%
filter(meteorologic_year == event$meteorologic_year) %>%
filter(gage == event$gage) %>%
nrow()
#Export info
tibble(
event_id = event$event_id,
freq_local = count
)
}
#run function
n.cores <- detectCores() - 1
cl <- makeCluster(n.cores)
clusterExport(cl, "df")
output<-parLapply(cl, seq(1,nrow(df)), fun)
remove(cl)
#add results to df
output<-bind_rows(output)
df<-left_join(df,output)
### Make rel_freq metric
df$rel_freq = df$freq_local/df$n
dat.scale <- df %>%
#Select vars of interest
select("peak2zero","drying_rate",
"dry_date_start", "dry_dur",
"peak_quantile", "rel_freq") %>%
#scale vars
scale()
dat.scale <- df %>%
#Select vars of interest
select("peak2zero","drying_rate"
, "dry_dur",
"peak_quantile", "rel_freq") %>%
#scale vars
scale()
############## PCA ############
PCA = prcomp(dat.scale)
autoplot(PCA,loadings=T,loadings.label=T)
wcke<-eclust(dat.scale, "kmeans", hc_metric="euclidean",k=5)
################### K-Means ######################
fviz_nbclust(dat.scale, kmeans, method = "silhouette") + theme_classic()
wcke<-eclust(dat.scale, "kmeans", hc_metric="euclidean",k=5)
